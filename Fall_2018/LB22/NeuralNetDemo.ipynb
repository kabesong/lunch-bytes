{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and test a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lunch Bytes (LB22) • October 26, 2018 • Matt Grossi (matthew.grossi at rsmas.miami.edu) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document provides a practical follow-up to my talk, *Peeking under the hood of an artificial neural network*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's set up a simple neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define functions for the routines we'll be using frequently. The workflow for [most of] these come from the presentation slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: There are undoubtedly far more efficient ways to carry out these operations. I've chosen overly descriptive variables names to be as descriptive (and hopefully as helpful) as possible. I have not thoroughly checked this over for correct performance, typos, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xfer(wsum):\n",
    "    out = 1.0 / (1.0 + np.exp(-wsum))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ErrHid(output, weights, outerr):\n",
    "    dt = np.dot(weights, outerr)\n",
    "    ErrorHid = output * (1.0 - output) * dt\n",
    "    return ErrorHid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ErrOut(output, targets):\n",
    "    ErrorOut = output * (1.0 - output) * (targets - output)\n",
    "    return ErrorOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WgtAdj(weights, responsibities, values, learnrate):\n",
    "    weights += (learnrate * np.outer(values, responsibilities))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiasAdj(bias, responsibilities, learnrate):\n",
    "    bias += (learn * responsibilities)\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(examples, targets, weightsI2H, weightsH2O, biasHID, biasOUT):\n",
    "    HiddenLayerInput = np.dot(examples, weightsI2H) + biasHID\n",
    "    HiddenLayerOutput = xfer(wsum=HiddenLayerInput)\n",
    "    OutputLayerInput = np.dot(HiddenLayerOutput, weightsH2O) + biasOUT\n",
    "    Output = xfer(wsum=OutputLayerInput)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nnet(examples, targets, hidden, learn):\n",
    "    \n",
    "    # Set number of attributes passed: number of columns in 'examples'\n",
    "    NumAtt = examples.shape[1]\n",
    "    \n",
    "    # Set number of output neurons: number of columns in 'targets'\n",
    "    NumOut = train_targets.shape[1]\n",
    "    \n",
    "    # Randomly initialize weight matrices (replace any zeros with 0.1)\n",
    "    weightsI2H = np.random.uniform(-1,1, size=(NumAtt,hidden))\n",
    "    weightsI2H[weightsI2H==0] = 0.1\n",
    "    weightsH2O = np.random.uniform(-1,1,size=(hidden,NumOut))\n",
    "    weightsH2O[weightsH2O==0] = 0.1\n",
    "    \n",
    "    # Randomly initialize biases for hidden and output layer neurons\n",
    "    biasHID = np.random.uniform(-1,1,size=hidden)\n",
    "    biasOUT = np.random.uniform(-1,1,size=NumOut)\n",
    "            \n",
    "    # Loop to pass each training example\n",
    "    for ex in range(len(examples)):\n",
    "            \n",
    "        # Forward propagate examples\n",
    "        HiddenLayerInput = np.dot(examples[ex,:], weightsI2H) + biasHID\n",
    "        HiddenLayerOutput = xfer(wsum=HiddenLayerInput)\n",
    "\n",
    "        OutputLayerInput = np.dot(HiddenLayerOutput, weightsH2O) + biasOUT\n",
    "        Output = xfer(wsum=OutputLayerInput)\n",
    "\n",
    "        # Back-proagate error: calculaterror responsibilities for output and hidden layer neurons\n",
    "        OutputErr = ErrOut(output=Output, targets=targets[ex,:])\n",
    "        HiddenErr = ErrHid(output=HiddenLayerOutput, weights=weightsH2O, outerr=OutputErr)\n",
    "\n",
    "        # Adjust weights\n",
    "        weightsI2H += learn * np.outer(examples[ex,:], HiddenErr)\n",
    "        weightsH2O += learn * np.outer(HiddenLayerOutput, OutputErr)\n",
    "        \n",
    "        biasHID += learn * HiddenErr\n",
    "        biasOUT += learn * OutputErr\n",
    "        \n",
    "        # Correctly classified?\n",
    "        if (train_targets[ex,0] < train_targets[ex,1]) == (Output[0] < Output[1]):\n",
    "            print('Correct')\n",
    "        else:\n",
    "            print('Incorrect')\n",
    "        \n",
    "    return weightsI2H, weightsH2O, biasHID, biasOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on a data set from the UC Irving Machine Learning Respository (https://archive.ics.uci.edu/ml/index.php). This handy respository contains hundreds of pre-classified data sets that are ideal for testing machine learning algorithms.\n",
    "\n",
    "Consider the **banknote authentication** data set. The documentation states that it contains 1372 examples of banknote-like specimens that are classified as either *authentic* (class 1) or *not authentic* (class 0) based on four attributes: variance, skewness, curtosis, and entropy of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Documents/Classes/RSMAS/MachLearn/Project1/banknote.csv\",\n",
    "                   names=['var', 'skew', 'curt', 'ent', 'class-yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>curt</th>\n",
       "      <th>ent</th>\n",
       "      <th>class-yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6216</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5459</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.8660</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      var    skew    curt      ent  class-yes\n",
       "0  3.6216  8.6661 -2.8073 -0.44699          0\n",
       "1  4.5459  8.1674 -2.4586 -1.46210          0\n",
       "2  3.8660 -2.6383  1.9242  0.10645          0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>curt</th>\n",
       "      <th>ent</th>\n",
       "      <th>class-yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.7503</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.7771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.5637</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.2823</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.5419</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.1952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         var      skew     curt     ent  class-yes\n",
       "1369 -3.7503 -13.45860  17.5932 -2.7771          1\n",
       "1370 -3.5637  -8.38270  12.3930 -1.2823          1\n",
       "1371 -2.5419  -0.65804   2.6842  1.1952          1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding a neural net, we need to set up our data. First, we note that these data are sorted by *class* (the last column, 0s and 1s), as is often the case with pre-classified data. It is good practice to shuffle the data to help ensure that the distribution of classes is roughly the same in both the training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of these data as having two classes: *authentic* and *not authentic*. Our data set already contains a column that contains 1s whenever the example is in the *authentic* class. Now let's add a column that has a 1 when the example is in the *not authentic* class. Then print the first few lines to make sure we have what we think we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['class-no'] = np.where(data['class-yes']==0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>skew</th>\n",
       "      <th>curt</th>\n",
       "      <th>ent</th>\n",
       "      <th>class-yes</th>\n",
       "      <th>class-no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.69078</td>\n",
       "      <td>-0.50077</td>\n",
       "      <td>-0.35417</td>\n",
       "      <td>0.474980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.55100</td>\n",
       "      <td>1.89550</td>\n",
       "      <td>0.18650</td>\n",
       "      <td>-2.440900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.45610</td>\n",
       "      <td>-4.55660</td>\n",
       "      <td>6.45340</td>\n",
       "      <td>-0.056479</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.12730</td>\n",
       "      <td>-7.11210</td>\n",
       "      <td>11.38970</td>\n",
       "      <td>-0.083634</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.22380</td>\n",
       "      <td>2.79350</td>\n",
       "      <td>0.32274</td>\n",
       "      <td>-0.860780</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       var     skew      curt       ent  class-yes  class-no\n",
       "0 -0.69078 -0.50077  -0.35417  0.474980          1         0\n",
       "1 -3.55100  1.89550   0.18650 -2.440900          1         0\n",
       "2 -2.45610 -4.55660   6.45340 -0.056479          1         0\n",
       "3 -3.12730 -7.11210  11.38970 -0.083634          1         0\n",
       "4 -3.22380  2.79350   0.32274 -0.860780          1         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to normalize our data such that all columns are between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_norm = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the data into training and testing subsets. Here we choose to use 80% of the examples for training and 20% for testing. Finally, separate into examples, which will contain the attributes, and the targets, which will contain the class flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data_norm, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_examples = np.array(train.iloc[:,0:4], dtype=np.float64)\n",
    "train_targets = np.array(train[['class-yes', 'class-no']], dtype=np.float64)\n",
    "\n",
    "test_examples = np.array(test.iloc[:,0:4], dtype=np.float64)\n",
    "test_targets = np.array(test[['class-yes', 'class-no']], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, replace targets 1 and 0 with 0.8 and 0.2. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_targets[np.where(train_targets==1)] = 0.8\n",
    "#train_targets[np.where(train_targets==0)] = 0.2\n",
    "\n",
    "#test_targets[np.where(test_targets==1)] = 0.8\n",
    "#test_targets[np.where(test_targets==0)] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the neural net! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Correct\n",
      "Correct\n",
      "Incorrect\n",
      "Incorrect\n",
      "Correct\n",
      "Incorrect\n"
     ]
    }
   ],
   "source": [
    "I2H, H2O, bHID, bOUT = nnet(examples=train_examples, targets=train_targets,\n",
    "                            hidden=5, learn=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instructive purposes, the function prints whether each example is correctly classified or not. One can see the variation in performance from example to example. Keep in mind that after each incorrect classification, slight adjustments are made to the internal weights.\n",
    "\n",
    "The function 'nnet' loops through every training example once, representing one training *epoch*. In practice, the network would be trained over many (sometimes hundreds) of epochs, depending on the number of examples and the complexity of the data. At the end of each epoch, the model is run on the testing data and the mean squared error (MSE) is calculated over all of these training examples to assess the model performance. As the model learns, the MSE should decrease.\n",
    "\n",
    "Just for fun, let's run our neural net on the test examples. Notice that 'nnet' outputs a weight matrix and bias vector for each layer. All we need to do to run the model is forward-propagate the testing examples using these final weights and biases. No weight adjustments will be made, because we are running a trained model. See the 'run' function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = run(examples=test_examples, targets=test_targets,\n",
    "          weightsI2H=I2H, weightsH2O=H2O, biasHID=bHID, biasOUT=bOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46995955, 0.55158251],\n",
       "       [0.44113467, 0.53328524],\n",
       "       [0.48101168, 0.52339657],\n",
       "       [0.47412868, 0.54076002],\n",
       "       [0.47010599, 0.54225907]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What do these numbers mean? Remember that our target matrix consists of 0.8s and 0.2s (originally 1s and 0s) and correspond to whether the example was in the class *authentic* or *not authentic*. Our neural network had 2 output neurons, one for each class. The columns of 'out' therefore represent these two classes, in the same order as in the 'data'.\n",
    "\n",
    "Because this is a simple classification problem, we can choose for each example the class (*i.e.*, column) with the higher number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicted classes\n",
    "predicted_classes = np.array(pd.DataFrame(out).idxmax(axis=1))\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0\n",
      " 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
      " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1\n",
      " 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1\n",
      " 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
      " 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Target classes\n",
    "target_classes = np.array(pd.DataFrame(test_targets).idxmax(axis=1))\n",
    "print(target_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that python uses zero-based indexing. Thus, a 0 here means the first column contains the highest number, meaning the example is authentic. This is admittedly a little confusing. How does this compare to the actual testing data? The neural net is correct if the predicted class agrees with the actual class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.90909090909091"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(target_classes == predicted_classes)/len(test_targets) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not well! In one epoch, the network is still pretty random. If the two classes are equally represented, then the network will achive 50% accuracy by predicting the same class every time, which is hardly impressive. Here it correctly classified 54% of the examples. But remember, this is only one epoch, and often many are needed to properly train a neural net.\n",
    "\n",
    "This data set is particularly clean and linearly separable, so it should be predicted with relative ease. Indeed, after 200 or so epochs, this simple neural net is able to achieve >99% accuracy on testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
